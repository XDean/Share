Artificial Neural Network
Gradient Descent and Back Propagation
1 Jul 2019
Tags: ANN, GD, BP

Dean Xu
ASML Brion
dean.xu@asml.com
github.com/XDean

* Regression Analysis 回归分析

世界由数据（值）和过程（函数），而过程又有白盒和黑盒。对于黑盒的过程，我们虽然能够得到输入输出却不能确切了解到系统内部。例如

```
给定一幅10*10的像素图，问图中是数字几。（下记作P问题）（并指代一系列人脑可解机器不可解的问题）
```

这个简单的问题所有人脑都是一个实现，但是没有人可以用数字符号精确描述人脑是如何做到的。如何解析人脑构造似乎是脑科学家的工作，而统计学家（数据科学家）想要的只是以最低成本解答这一问题。这便开启了统计学回归分析的大门。(分类也可以看作是离散回归的一种，后文不作区分)

![unknown-system-fitting.png](unknown-system-fitting.png)

* Classic Regression methods 经典回归方法

- 在高中数学课本中我们都会有“最小二乘法”一节，学习的就是最简单的线性回归。我们构造一个线性过程来模拟目标过程，并用最小二乘法寻找最优解。
- 在高中生物选修课本中我们又会了解到用逻辑函数(Logistic function)来模拟预测生物种群数量变化
- 在大学概率论课程中我们又会学习到用泊松分布来模拟预测随机变量的分布和间隔

自19世纪初高斯发明改进最小二乘法以来，200年间有许许多多的回归方法被发明。但是很可惜的是，这些方法都难以解决P问题。因为各种方法是针对给定条件建立的特异性模型，这也是“数学建模”这一分支的职责。那么敢问数学建模专家，P问题该如何建模呢？答案不言而喻，当然是我们的主角，神经网络。

* Neural Network History 神经网络简史

对于P问题的解答科学家们从未停止研究。自1906年神经元研究获得诺贝尔奖，37年后的1943年，美国神经学家Warren McCulloch和逻辑学家Walter Pitts首次提出了模拟神经元的M-P模型。他们的思想很简单，既然人脑可以解决P问题，我们为什么不模拟人脑的思考方式呢？

![MP-model.jpg](MP-model.jpg)

这就是神经网络的雏形。其主要可以用来解决逻辑运算和线性分类问题。

6年后，加拿大心理学家Donald Hebb提出了赫布学习方法来调整神经元之间的关联。他认为两个神经元若总是同时被激发，则其中之一会促进另一个的激发，类似于生物条件反射的形成。其调整方法如下

$\Delta\omega_{ij}=\eta\cdot a_i\cdot o_j$

即 参数变化 = 学习效率 x 输出神经元输入值 x 输入神经元输出值。赫布学习方法即是最早的神经网络训练方法。

* Neural Network History 神经网络简史

1954年有人首次使用计算机模拟了一个赫布网络，4年后神经网络终于迎来了真正的应用，美国神经科学家Frank Rosenblatt对M-P模型进行改进，发明了感知机(Perceptron)，成功的在IBM-704上完成了感知机并用两年的时间实现了英文字母数字的识别。同期，还有斯坦福教授Bernard Widrow开发的Adaline网络。

![adaline.png](adaline.png)

自此，神经网络领域进入了长达10年的发展阶段。
但是渐渐地人们发现（单层）神经网络能够处理的问题局限在二元线性可分问题，面对非线性问题，连最基本的异或问题都无法解决。
在10年发展后神经网络陷入了15年无人问津的低迷期。

虽然接下来要着重介绍的反向传播方法(1974 社会科学家 Paul Werbos)正是出现在这一低迷期，但没有受到重视。
